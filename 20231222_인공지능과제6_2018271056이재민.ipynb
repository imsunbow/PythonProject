{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/imsunbow/PythonProject/blob/main/20231222_%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5%EA%B3%BC%EC%A0%9C6_2018271056%EC%9D%B4%EC%9E%AC%EB%AF%BC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "TJF4tEn2EoS5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a000720-f2b8-4521-ae78-029daa787490"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Q table is\n",
            "\n",
            "col=0       1        2        3       4         5\n",
            "row: 0\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            "row: 1\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            "row: 2\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            "row: 3\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            "row: 4\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            "row: 5\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            "row: 6\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            "row: 7\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            "row: 8\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            "\n",
            "Learning starts.\n",
            "\n",
            "episode= 0   epsilon= 0.4\n",
            "episode= 5000   epsilon= 0.36924653855465434\n",
            "episode= 10000   epsilon= 0.3408575155864846\n",
            "episode= 15000   epsilon= 0.3146511444266214\n",
            "episode= 20000   epsilon= 0.2904596148294764\n",
            "episode= 25000   epsilon= 0.26812801841425576\n",
            "episode= 30000   epsilon= 0.24751335672245633\n",
            "episode= 35000   epsilon= 0.22848362553952595\n",
            "episode= 40000   epsilon= 0.21091696961721942\n",
            "episode= 45000   epsilon= 0.19470090238398868\n",
            "episode= 50000   epsilon= 0.17973158564688865\n",
            "episode= 55000   epsilon= 0.16591316467263256\n",
            "episode= 60000   epsilon= 0.15315715439004485\n",
            "episode= 65000   epsilon= 0.14138187278351208\n",
            "episode= 70000   epsilon= 0.1305119178492158\n",
            "episode= 75000   epsilon= 0.12047768476488085\n",
            "episode= 80000   epsilon= 0.11121492018127765\n",
            "episode= 85000   epsilon= 0.10266431078142238\n",
            "episode= 90000   epsilon= 0.09477110347284871\n",
            "episode= 95000   epsilon= 0.08748475478088591\n",
            "\n",
            "Learning is finished. The Q table is:\n",
            "\n",
            "col=0       1        2        3       4         5\n",
            "row: 0\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            "row: 1\n",
            " 0.00,   -9.00,   -9.00,   -9.00,   -9.00,   -9.00,   -9.00,   -9.00,    0.00\n",
            " 0.00,    4.30,    4.78,    5.31,    5.90,    5.31,    4.78,   -9.00,    0.00\n",
            " 0.00,    4.30,    4.78,   -9.00,   -9.00,    6.56,   -9.00,    4.30,    0.00\n",
            " 0.00,   -9.00,    3.87,    4.30,    4.78,    5.31,    5.90,    5.31,    0.00\n",
            "row: 2\n",
            " 0.00,    3.87,    4.30,    0.00,    0.00,    5.90,    0.00,    4.78,    0.00\n",
            " 0.00,    4.78,   -9.00,    0.00,    0.00,   -9.00,    0.00,   -9.00,    0.00\n",
            " 0.00,    4.78,    5.31,    0.00,    0.00,    7.29,    0.00,    3.87,    0.00\n",
            " 0.00,   -9.00,    4.30,    0.00,    0.00,   -9.00,    0.00,   -9.00,    0.00\n",
            "row: 3\n",
            " 0.00,    4.30,    4.78,   -9.00,   -9.00,    6.56,   -9.00,    4.30,    0.00\n",
            " 0.00,    5.31,    5.90,    6.56,    7.29,    6.56,    5.90,   -9.00,    0.00\n",
            " 0.00,    4.30,   -9.00,    5.31,   -9.00,    8.10,    7.29,   -9.00,    0.00\n",
            " 0.00,   -9.00,    4.78,    5.31,    5.90,    6.56,    7.29,    6.56,    0.00\n",
            "row: 4\n",
            " 0.00,    4.78,    0.00,    5.90,    0.00,    7.29,    6.56,    0.00,    0.00\n",
            " 0.00,   -9.00,    0.00,   -9.00,    0.00,    7.29,   -9.00,    0.00,    0.00\n",
            " 0.00,    3.87,    0.00,    4.78,    0.00,    9.00,    8.10,    0.00,    0.00\n",
            " 0.00,   -9.00,    0.00,   -9.00,    0.00,   -9.00,    8.10,    0.00,    0.00\n",
            "row: 5\n",
            " 0.00,    4.30,   -9.00,    5.31,    0.00,    0.00,    7.29,   -9.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    7.29,   -9.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    7.29,    6.56,    0.00\n",
            " 0.00,   -9.00,    3.87,    0.00,    0.00,    0.00,    9.00,    8.10,    0.00\n",
            "row: 6\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    8.10,    7.29,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    6.56,   -9.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    6.56,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00,   -9.00,    7.29,    0.00\n",
            "row: 7\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    7.29,    6.56,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    5.90,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            "row: 8\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            "\n",
            "Test starts.\n",
            "\n",
            "\n",
            "Episode: 0  start state: ( 1 ,  1 )\n",
            "Episode has ended. Total reward received in episode =  9\n",
            "\n",
            "Episode: 1  start state: ( 1 ,  1 )\n",
            "Episode has ended. Total reward received in episode =  9\n",
            "\n",
            "Episode: 2  start state: ( 1 ,  1 )\n",
            "Episode has ended. Total reward received in episode =  9\n",
            "\n",
            "Episode: 3  start state: ( 1 ,  1 )\n",
            "Episode has ended. Total reward received in episode =  9\n",
            "\n",
            "Episode: 4  start state: ( 1 ,  1 )\n",
            "Episode has ended. Total reward received in episode =  9\n",
            "Program ends!!!\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import time\n",
        "import random\n",
        "import math\n",
        "from datetime import datetime\n",
        "import tensorflow as tf\n",
        "\n",
        "total_episodes = 100000\n",
        "max_steps = 99\n",
        "gamma = 0.9\n",
        "alpha = 1.0\n",
        "original_epsilon = 0.4\n",
        "decay_rate = 0.000016\n",
        "random.seed(datetime.now().timestamp())\n",
        "\n",
        "max_row = 9\n",
        "max_col = 9\n",
        "max_num_actions = 4\n",
        "\n",
        "env_state_space = [\n",
        "    ['H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H'],\n",
        "    ['H', 'S', 'F', 'F', 'F', 'F', 'F', 'F', 'H'],\n",
        "    ['H', 'F', 'F', 'H', 'H', 'F', 'H', 'F', 'H'],\n",
        "    ['H', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'H'],\n",
        "    ['H', 'F', 'H', 'F', 'H', 'F', 'F', 'H', 'H'],\n",
        "    ['H', 'F', 'F', 'F', 'F', 'G', 'F', 'F', 'H'],\n",
        "    ['H', 'F', 'H', 'H', 'F', 'H', 'F', 'F', 'H'],\n",
        "    ['H', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'H'],\n",
        "    ['H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H']\n",
        "]\n",
        "\n",
        "Q = np.zeros((max_row, max_col, max_num_actions))\n",
        "move_offset = [[-1, 0], [0, 1], [1, 0], [0, -1]]\n",
        "move_str = ['up   ', 'right', 'down ', 'left ']\n",
        "\n",
        "class ExperienceReplayBuffer:\n",
        "    def __init__(self, size):\n",
        "        self.buffer = []\n",
        "        self.size = size\n",
        "\n",
        "    def add(self, experience):\n",
        "        self.buffer.append(experience)\n",
        "        if len(self.buffer) > self.size:\n",
        "            self.buffer.pop(0)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        if len(self.buffer) < batch_size:\n",
        "            return []\n",
        "\n",
        "        return random.sample(self.buffer, batch_size)\n",
        "\n",
        "def choose_action_with_epsilon_greedy(s, epsilon):\n",
        "    r = s[0]\n",
        "    c = s[1]\n",
        "    q_a_list = Q[r, c, :]\n",
        "    max_action = np.argmax(q_a_list)\n",
        "    rn = random.random()\n",
        "\n",
        "    if rn >= epsilon:\n",
        "        action = max_action\n",
        "    else:\n",
        "        rn1 = random.random()\n",
        "        if rn1 >= 0.75:\n",
        "            action = 0\n",
        "        elif rn1 >= 0.5:\n",
        "            action = 1\n",
        "        elif rn1 >= 0.25:\n",
        "            action = 2\n",
        "        else:\n",
        "            action = 3\n",
        "    return action\n",
        "\n",
        "def choose_action_with_greedy(s):\n",
        "    r = s[0]\n",
        "    c = s[1]\n",
        "    q_a_list = Q[r, c, :]\n",
        "    max_action = np.argmax(q_a_list)\n",
        "    return max_action\n",
        "\n",
        "def get_new_state_and_reward(s, a):\n",
        "    new_state = [s[0] + move_offset[a][0], s[1] + move_offset[a][1]]\n",
        "    cell = env_state_space[new_state[0]][new_state[1]]\n",
        "\n",
        "    if cell == 'F':\n",
        "        rew = 0\n",
        "    elif cell == 'H':\n",
        "        rew = -9\n",
        "    elif cell == 'G':\n",
        "        rew = 9\n",
        "    elif cell == 'S':\n",
        "        rew = 0\n",
        "    else:\n",
        "        print(\"Logic error in get_new_state_and_reward. This cannot happen!\")\n",
        "        return [0, 0], -20000\n",
        "\n",
        "    return new_state, rew\n",
        "\n",
        "def display_Q_table(Q):\n",
        "    print(\"\\ncol=0       1        2        3       4         5\")\n",
        "    for r in range(max_row):\n",
        "        print(\"row:\", r)\n",
        "        for a in range(max_num_actions):\n",
        "            line = \",   \".join(\"{:5.2f}\".format(Q[r, c, a]) for c in range(max_col))\n",
        "            print(line)\n",
        "\n",
        "def env_rendering(s):\n",
        "    for i in range(0, max_row, 1):\n",
        "        line = \"\"\n",
        "        for j in range(0, max_col, 1):\n",
        "            line = line + env_state_space[i][j]\n",
        "        if s[0] == i:\n",
        "            col = s[1]\n",
        "            line1 = line[:col] + '*' + line[col + 1:]\n",
        "        else:\n",
        "            line1 = line\n",
        "        print(line1)\n",
        "\n",
        "def update_target_network(source_model, target_model):\n",
        "    target_model.set_weights(source_model.get_weights())\n",
        "\n",
        "# Neural Network Model\n",
        "class QNetwork(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super(QNetwork, self).__init__()\n",
        "        self.flatten = tf.keras.layers.Flatten(input_shape=(max_row, max_col))\n",
        "        self.dense1 = tf.keras.layers.Dense(128, activation='relu')\n",
        "        self.dense2 = tf.keras.layers.Dense(64, activation='relu')\n",
        "        self.output_layer = tf.keras.layers.Dense(max_num_actions, activation='linear')\n",
        "\n",
        "    def call(self, state, training=False):\n",
        "        x = self.flatten(state)\n",
        "        x = self.dense1(x)\n",
        "        x = self.dense2(x)\n",
        "        return self.output_layer(x)\n",
        "\n",
        "# Experience Replay Buffer\n",
        "replay_buffer = ExperienceReplayBuffer(size=1000)\n",
        "\n",
        "# Q Networks\n",
        "q_network = QNetwork()\n",
        "target_q_network = QNetwork()\n",
        "update_target_network(q_network, target_q_network)\n",
        "\n",
        "# Loss and Optimizer\n",
        "mse_loss = tf.keras.losses.MeanSquaredError()\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "# Training\n",
        "print(\"Initial Q table is\")\n",
        "display_Q_table(Q)\n",
        "\n",
        "start_state = [1, 1]\n",
        "\n",
        "print(\"\\nLearning starts.\\n\")\n",
        "for episode in range(total_episodes):\n",
        "    S = start_state\n",
        "    epsilon = original_epsilon * math.exp(-decay_rate * episode)\n",
        "\n",
        "    if episode % 5000 == 0:\n",
        "        print('episode=', episode, '  epsilon=', epsilon)\n",
        "\n",
        "    for step in range(max_steps):\n",
        "        A = choose_action_with_epsilon_greedy(S, epsilon)\n",
        "        S_, R = get_new_state_and_reward(S, A)\n",
        "\n",
        "        r = S[0]\n",
        "        c = S[1]\n",
        "\n",
        "        Q[r][c][A] = Q[r][c][A] + alpha * (R + gamma * np.max(Q[S_[0]][S_[1]][:]) - Q[r][c][A])\n",
        "\n",
        "        replay_buffer.add((S, A, R, S_))\n",
        "\n",
        "        S = S_\n",
        "\n",
        "        if env_state_space[S[0]][S[1]] == 'G' or env_state_space[S[0]][S[1]] == 'H':\n",
        "            break\n",
        "\n",
        "\n",
        "if episode % 100 == 0 and len(replay_buffer.buffer) >= 32:\n",
        "    sampled_batch = replay_buffer.sample(32)\n",
        "\n",
        "    if sampled_batch:  # Check if sampled_batch is not empty\n",
        "        states, actions, rewards, next_states = zip(*sampled_batch)\n",
        "\n",
        "        states = np.array(states)\n",
        "        actions = np.array(actions)\n",
        "        rewards = np.array(rewards)\n",
        "        next_states = np.array(next_states)\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            target_q_values = target_q_network(next_states, training=False).numpy()\n",
        "            max_next_q_values = np.max(target_q_values, axis=1)\n",
        "            target_q_values = rewards + gamma * max_next_q_values\n",
        "\n",
        "            q_values = q_network(states, training=True)\n",
        "            q_values = tf.reduce_sum(q_values * tf.one_hot(actions, max_num_actions), axis=1)\n",
        "\n",
        "            loss = mse_loss(target_q_values, q_values)\n",
        "\n",
        "        gradients = tape.gradient(loss, q_network.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, q_network.trainable_variables))\n",
        "\n",
        "        update_target_network(q_network, target_q_network)\n",
        "\n",
        "print('\\nLearning is finished. The Q table is:')\n",
        "display_Q_table(Q)\n",
        "\n",
        "# Test stage\n",
        "print(\"\\nTest starts.\\n\")\n",
        "\n",
        "for e in range(5):\n",
        "    S = start_state\n",
        "    total_rewards = 0\n",
        "    print(\"\\nEpisode:\", e, \" start state: (\", S[0], \", \", S[1], \")\")\n",
        "\n",
        "    for step in range(max_steps):\n",
        "        A = choose_action_with_greedy(S)\n",
        "        S_, R = get_new_state_and_reward(S, A)\n",
        "\n",
        "        total_rewards += R\n",
        "        S = S_\n",
        "\n",
        "        if env_state_space[S[0]][S[1]] == 'G' or env_state_space[S[0]][S[1]] == 'H':\n",
        "            break\n",
        "\n",
        "    print(\"Episode has ended. Total reward received in episode = \", total_rewards)\n",
        "    time.sleep(1)\n",
        "\n",
        "print(\"Program ends!!!\")\n"
      ]
    }
  ]
}