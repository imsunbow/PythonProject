{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WcT5ImQ80zde"
      },
      "outputs": [],
      "source": [
        "# 필요 라이브러리 import\n",
        "import os\n",
        "import time\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 상수 선언\n",
        "d_embed = 100     ## word embedding vector 의 길이 (원소수)\n",
        "Max_seq_length = 128    ## time step 수 (MSL)\n",
        "Vocab_size = 51459\t# 사전 Vocab 의 총 단어수\n",
        "Num_POS = 50    ## 총 품사 수\n",
        "LEARNING_RATE = 0.7e-4\n",
        "EPOCHS = 20\n",
        "BATCH_SIZE = 64"
      ],
      "metadata": {
        "id": "hQ4lG3DF8k_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GcWy-rUk8sGz",
        "outputId": "e315ca6c-563b-4203-cdf3-1f55735c3c38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 단어와 그 출현횟수를 dict 타입의 사전에 수집한다.\n",
        "def build_vocabulary_temp(corpus_path):\n",
        "    fp = open(corpus_path, \"r\", encoding=\"utf-8\")\n",
        "    for line in fp.readlines():\n",
        "        sentence = line.split()\n",
        "        if sentence[0] == '<<':\n",
        "            continue\n",
        "        for word_pos_pair in sentence:\n",
        "            w_p = word_pos_pair.split('/')\n",
        "            nseg = len(w_p)\n",
        "            ## word/pos 내에 슬래시가 2개 이상 있어 3 조각 이상이 나옴.\n",
        "            if nseg > 2:    ## 마지막 슬래시를 기준으로 단어와 품사로 구분함.\n",
        "                word = ''\n",
        "                for i in range(nseg-1):\n",
        "                    word = word + w_p[i] + '/'\n",
        "                word = word[:-1]    # remove the last slash.\n",
        "            else:\n",
        "                word = w_p[0]\n",
        "\n",
        "            pos = w_p[-1]   # the last segment\n",
        "            if word in Vocab_temporary:\n",
        "                Vocab_temporary[word] += 1\n",
        "            else:\n",
        "                Vocab_temporary[word] = 1\n",
        "    fp.close()\n",
        "    return"
      ],
      "metadata": {
        "id": "6u15Gqgj8yfQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1 차적인 임시사전을 만든다. 사전명: Vocab_temporary\n",
        "# key: 단어, value:출현횟수\n",
        "# 수집할 단어들의 대상은 penn-tree-bank 내의 모든 파일에 존재하는 단어들이다!!\n",
        "\n",
        "Vocab_temporary = {}\n",
        "build_vocabulary_temp(\"./drive/MyDrive/all_word_pos_sentences_all.txt\")\n",
        "\n",
        "# 사전 단어들을 출현횟수로 내림차순으로 정렬하여 그 결과를 리스트로 받는다.\n",
        "sorted_Vocab = sorted(Vocab_temporary.items(), key = lambda kv: kv[1], reverse=True)\n",
        "Total_n_words = len(sorted_Vocab)\n",
        "print('파일에서 모은 총 단어수:', Total_n_words)"
      ],
      "metadata": {
        "id": "kmVG0j0482Vk",
        "outputId": "becb076f-81c6-4134-b6d8-41c08d59f314",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "파일에서 모은 총 단어수: 51457\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 두 개의 특수단어('[PAD]', '[UNK]')를 포함하는 정식 사전(사전명: Vocab) 을 만든다:\n",
        "#     key: 단어, value: 단어번호\n",
        "Vocab = {}\n",
        "Vocab['[PAD]'] = 0  # 특수단어 추가\n",
        "Vocab['[UNK]'] = 1  # 특수단어 추가\n",
        "# Vocab_temporary 에 모은 단어들에게는 단어번호를 2 부터 준다.\n",
        "for i in range(Total_n_words):\n",
        "    word = sorted_Vocab[i][0]\n",
        "    freq = sorted_Vocab[i][1]\n",
        "    Vocab[word] = i + 2\n",
        "\n",
        "Total_number_words = len(Vocab)\n",
        "print(\"최종 사전의 총 단어수:\", Total_number_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EOfj-eXF9JbQ",
        "outputId": "4ecb5536-a11a-4dc3-a8a3-8a3e965435fc"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "최종 사전의 총 단어수: 51459\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## 역-단어사전 i_Vocab  만들기:  Vocab 에서 key와 value 를 바꾼 사전.\n",
        "#    key: 단어번호,  value: 단어\n",
        "all_wps = list(Vocab.keys())\n",
        "i_Vocab = {}\n",
        "for word in all_wps:\n",
        "  widx = Vocab[word]\n",
        "  i_Vocab[widx] = word"
      ],
      "metadata": {
        "id": "4tlNJUmL9Pit"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 품사 사전 (사전명: dic_POS)만들기:  key:품사명,  value: 품사번호\n",
        "#    두 특수단어에게는 품사명/품사번호를  '[PAD]': 0,     '[UNK]': 1 로 준다.\n",
        "#    나머지는 penn-tree-bank 의 48 개의 품사들에게 품사 번호를 2 부터 부여한다(2~49)\n",
        "#    결국 총 품사는 총 50 개로 번호는 0 ~ 49 가 된다.\n",
        "all_pos_list = ['[PAD]', '[UNK]',\n",
        "                'CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS',\n",
        "                'MD', 'NN', 'NNS', 'NNP', 'NNPS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB',\n",
        "                'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN',\n",
        "                'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB', '#', '$', '.', ',',\n",
        "                ':', '(', ')', '\\'\\'', '\\'', '``', '&rsquo', '”']\n",
        "dic_POS = {}\n",
        "for i in range(len(all_pos_list)):\n",
        "    dic_POS[all_pos_list[i]] = i\n",
        "\n",
        "num_pos = len(dic_POS)\n",
        "print(\"총 품사수: \", num_pos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tTsKV2rV-E82",
        "outputId": "6d211ed9-0970-4416-857f-cdd3fad6df74"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "총 품사수:  50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## 역-품사사전 만들기: dic_POS 에서 key와 value 를 바꾼 사전.\n",
        "#     key: 품사번호,     value: 품사명\n",
        "all_pos = list(dic_POS.keys())\n",
        "i_dic_POS = {}\n",
        "for a_pos in all_pos:\n",
        "  pidx = dic_POS[a_pos]\n",
        "  i_dic_POS[pidx] = a_pos\n",
        "\n",
        "def build_index_sentences(path_word_pos_sentence_file, path_index_sentence_file):\n",
        "    fp = open(path_word_pos_sentence_file, \"r\", encoding=\"utf-8\")\n",
        "    fp_w = open(path_index_sentence_file, \"w\", encoding=\"utf-8\")\n",
        "\n",
        "    for line in fp.readlines():\n",
        "        sentence = line.split()\n",
        "        if sentence[0] == '<<':   ## 파일명을 가지는 줄은 무시한다.\n",
        "            continue\n",
        "\n",
        "        line_widx = ''  # line for word indices\n",
        "        line_pidx = ''  # line for pos indices\n",
        "\n",
        "        for word_pos_pair in sentence:\n",
        "            w_p = word_pos_pair.split('/')\n",
        "            nseg = len(w_p)\n",
        "            if nseg > 2:\n",
        "                word = ''\n",
        "                for i in range(nseg-1):\n",
        "                    word = word + w_p[i] + '/'\n",
        "                word = word[:-1]    # remove the last slash.\n",
        "            else:\n",
        "                word = w_p[0]\n",
        "\n",
        "            pos = w_p[-1]   # the last segment\n",
        "            if not(word in Vocab):  # Other scheme: Vocab.get(word) 가 None 이면 없는 것을 맗암.\n",
        "                widx = 1    # give index of [UNK] since it is missing in Vocab.\n",
        "            else:\n",
        "                widx = Vocab[word]\n",
        "\n",
        "            if not(pos in dic_POS):\n",
        "                pos_list = pos.split('|')\n",
        "                pos = pos_list[-1]\n",
        "                if not (pos in dic_POS):\n",
        "                    print(\"exception occurs at dic_POS look_up. w_p=\", w_p, \" pos=\", pos)\n",
        "                    time.sleep(100)\n",
        "                else:\n",
        "                    pidx = dic_POS[pos]\n",
        "            else:\n",
        "                pidx = dic_POS[pos]\n",
        "\n",
        "            if len(line_widx) == 0:\n",
        "                line_widx = line_widx + str(widx)\n",
        "            else:\n",
        "                line_widx = line_widx + '\\t' + str(widx)\n",
        "\n",
        "            if len(line_pidx) == 0:\n",
        "                line_pidx = line_pidx + str(pidx)\n",
        "            else:\n",
        "                line_pidx = line_pidx + '\\t' + str(pidx)\n",
        "\n",
        "        fp_w.write(line_widx + '\\n')\n",
        "        fp_w.write(line_pidx + '\\n')\n",
        "        fp_w.write('\\n')    # an empty line after each sentence\n",
        "    fp_w.close()\n",
        "    fp.close()"
      ],
      "metadata": {
        "id": "BAJQElcm-GLA"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "## 단어/품사 문자열을 이용하는 훈련데이타 파일들로 부터  단어번호/품사번호를 이용하는 훈련데이타 파일들을 만든다.\n",
        "build_index_sentences(\"./drive/MyDrive/all_word_pos_sentences_train.txt\", \"./drive/MyDrive/all_index_sentences_train.txt\")\n",
        "build_index_sentences(\"./drive/MyDrive/all_word_pos_sentences_validation.txt\", \"./drive/MyDrive/all_index_sentences_validation.txt\")\n",
        "build_index_sentences(\"./drive/MyDrive/all_word_pos_sentences_test.txt\", \"./drive/MyDrive/all_index_sentences_test.txt\")\n",
        "\n",
        "\n",
        "##  훈련예제 준비 함수\n",
        "##   파일경로를 입력으로 받는다. 이 파일은 all_index_sentences_???.txt 이다.\n",
        "##  출력: 파일내의 모든 문장들에 대한 정보를 이용하여 다음을 준비하여 출력한다.\n",
        "##       1) list_X: 문장들의 단어 번호 리스트를 원소로 가지는 리스트\n",
        "##       2) list_Y: 문장들의 정답품사번호 리스트를 원소로 가지는 리스트\n",
        "##       3) list_leng: 문장들의 길이를 원소로 가지는 리스트\n",
        "\n",
        "def load_X_and_Y(path_index_file):\n",
        "    fp= open(path_index_file, \"r\", encoding=\"utf-8\")\n",
        "    list_X = []\n",
        "    list_Y = []\n",
        "    list_leng = []\n",
        "\n",
        "    while True:\n",
        "        # read two lines\n",
        "        wordline = fp.readline()\n",
        "        line_leng = len(wordline)\n",
        "\n",
        "        if line_leng == 0:\n",
        "            break   # end of file has come.\n",
        "        if line_leng == 1:\n",
        "            continue    # empty line used as sentence delimeter\n",
        "\n",
        "        # The line read just before is a line of word indices.\n",
        "        # The next line should be the corresponding pos index line.\n",
        "        posline = fp.readline()\n",
        "        w_index = wordline.split()\n",
        "        p_index = posline.split()\n",
        "\n",
        "        # X : a list of indices of words in a sentence.\n",
        "        # Y : a list of pos indices of words in the sentence of X.\n",
        "        X = []\n",
        "        Y = []\n",
        "\n",
        "        leng = len(w_index)\n",
        "        if leng > Max_seq_length:\n",
        "            leng = Max_seq_length   # truncation is done.\n",
        "\n",
        "        for i in range(leng):\n",
        "            X.append(int(w_index[i]))\n",
        "            Y.append(int(p_index[i]))\n",
        "\n",
        "        # pads are added after sentence\n",
        "        if leng < Max_seq_length:\n",
        "            for i in range(leng, Max_seq_length):\n",
        "                X.append(0)     # word index of '[PAD]' which is 0 is added.\n",
        "                Y.append(0)     # pos index of  '[PAD]' which is 0 is added.\n",
        "\n",
        "        list_X.append(X)\n",
        "        list_Y.append(Y)\n",
        "        list_leng.append(leng)\n",
        "\n",
        "    fp.close()\n",
        "    return list_X, list_Y, list_leng"
      ],
      "metadata": {
        "id": "H9LvJNPo-REp"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, y_train, leng_train = load_X_and_Y(\"./drive/MyDrive/all_index_sentences_train.txt\")\n",
        "x_train = np.array(x_train, dtype='i')\n",
        "y_train = np.array(y_train, dtype='i')\n",
        "\n",
        "x_validation, y_validation, leng_valiation = load_X_and_Y(\"./drive/MyDrive/all_index_sentences_validation.txt\")\n",
        "x_validation = np.array(x_validation, dtype='i')\n",
        "y_validation = np.array(y_validation, dtype='i')\n",
        "\n",
        "x_test, y_test, leng_test = load_X_and_Y(\"./drive/MyDrive/all_index_sentences_test.txt\")\n",
        "x_test = np.array(x_test, dtype='i')\n",
        "y_test = np.array(y_test, dtype='i')"
      ],
      "metadata": {
        "id": "aLC9aAnZ_kUl"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model 설계\n",
        "\n",
        "model = tf.keras.models.Sequential()\n",
        "\n",
        "## 층0:  word-embedding 층\n",
        "model.add(tf.keras.layers.Embedding(Vocab_size, d_embed, embeddings_initializer='random_normal', \\\n",
        "\tinput_length=Max_seq_length, mask_zero=True, trainable=True))\t# output shape: (bsz, MSL, d_emb)\n",
        "\n",
        "## 충1: LSTM 층\n",
        "model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256, return_sequences=True), input_shape=(Max_seq_length, d_embed)))\n",
        "\t# output shape: (batch_sz, MSL, 512)\n",
        "\n",
        "## 충2: LSTM 층\n",
        "model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True), input_shape=(Max_seq_length, 512)))\n",
        "\t# output shape: (batch_sz, MSL, 256)\n",
        "\n",
        "## 충3: LSTM 층\n",
        "model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True), input_shape=(Max_seq_length, 256)))\n",
        "\t# output shape: (batch_sz, MSL, 128)\n",
        "\n",
        "## 층4: NN 층\n",
        "model.add(tf.keras.layers.Dense(units=Num_POS, activation='softmax', use_bias=True))\t# 최종출력층. 각 시간의 각 단어마다 num_POS=50개의 확률이 생성됨.\n",
        "\t# output shape: (batch_sz, MSL, num_POS)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
        "model.compile(optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", metrics=['acc'])\n",
        "model.fit(x=x_train, y=y_train, batch_size=BATCH_SIZE, epochs=EPOCHS, validation_data=(x_validation, y_validation), shuffle=True, verbose=1)\n",
        "\n",
        "print(\"...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "qqsc7YUb_qfy",
        "outputId": "91d6c496-e872-4b06-82de-e0247058ac0e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "118/844 [===>..........................] - ETA: 47:14 - loss: 3.4278 - acc: 0.1202"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-fa93c4d625c0>\u001b[0m in \u001b[0;36m<cell line: 27>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLEARNING_RATE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"sparse_categorical_crossentropy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_validation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_validation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1781\u001b[0m                         ):\n\u001b[1;32m   1782\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1783\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1784\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    829\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 831\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    832\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    833\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    865\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 867\u001b[0;31m       return tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    868\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1262\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1263\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1264\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflat_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1265\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1266\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mflat_call\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    215\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mflat_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0;34m\"\"\"Calls with tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    253\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1477\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1478\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1479\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1480\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1481\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     ]\n\u001b[0;32m---> 60\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     61\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##### TEST ####################\n",
        "pred = model.predict(x=x_test, verbose=1)\n",
        "pred_label = tf.math.argmax(pred, axis=2)\n",
        "pred_label = pred_label.numpy()\n",
        "\n",
        "num_test_sentences = y_test.shape[0]\n",
        "\n",
        "# 정확도 계산\n",
        "correct_predictions = 0\n",
        "total_predictions = 0\n",
        "\n",
        "for i in range(num_test_sentences):\n",
        "    for j in range(Max_seq_length):\n",
        "        if y_test[i][j] == 0:  # [PAD] 토큰은 제외\n",
        "            continue\n",
        "        if pred_label[i][j] == y_test[i][j]:\n",
        "            correct_predictions += 1\n",
        "        total_predictions += 1\n",
        "\n",
        "# 테스트 데이터셋의 첫 20 문장에 대해 품사 인식 결과 출력\n",
        "num_sentences_to_display = 20\n",
        "\n",
        "for sentence_idx in range(num_sentences_to_display):\n",
        "    print(f\"({sentence_idx + 1}) \", end=\"\")  # 문장 번호 출력\n",
        "    for word_idx in range(Max_seq_length):\n",
        "        # PAD 토큰은 무시\n",
        "        if y_test[sentence_idx][word_idx] == 0:\n",
        "            break\n",
        "\n",
        "        # 실제 단어와 품사\n",
        "        actual_word = i_Vocab[x_test[sentence_idx][word_idx]]\n",
        "        actual_pos = i_dic_POS[y_test[sentence_idx][word_idx]]\n",
        "\n",
        "        # 예측된 품사\n",
        "        predicted_pos = i_dic_POS[pred_label[sentence_idx][word_idx]]\n",
        "\n",
        "        # 출력 포맷: 올바른 경우 - 단어/품사, 잘못된 경우 - 단어/잘못된품사<정답품사>\n",
        "        if actual_pos == predicted_pos:\n",
        "            print(f\"{actual_word}/{predicted_pos} \", end=\"\")\n",
        "        else:\n",
        "            print(f\"{actual_word}/{predicted_pos}<{actual_pos}> \", end=\"\")\n",
        "\n",
        "    print(\"\\n\")  # 문장이 끝나면 줄바꿈\n",
        "\n",
        "\n",
        "\n",
        "acc = correct_predictions / total_predictions\n",
        "print(\"Test Accuracy: \", acc)\n",
        "\n",
        "print(\"Program ends.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZrGrRIP1_5TO",
        "outputId": "4f938ead-172c-4bae-b565-c7da95768900"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "240/240 [==============================] - 159s 662ms/step\n",
            "(1) Arthur/IN<NNP> M./IN<NNP> Goldberg/NN<NNP> said/NN<VBD> he/NN<PRP> extended/NN<VBD> his/NN<PRP$> unsolicited/NN<JJ> tender/NN offer/NN of/NN<IN> $/NN<$> 32/NN<CD> \n",
            "\n",
            "(2) a/DT share/NN tender/NN offer/NN ,/NN<,> or/NN<CC> $/NN<$> 154.3/NN<CD> million/NN<CD> ,/NN<,> for/NN<IN> Di/NN<NNP> Giorgio/NN<NNP> Corp./NN<NNP> to/NN<TO> Nov./NN<NNP> 1/NN<CD> ./NN<.> \n",
            "\n",
            "(3) DIG/DT<NNP> Acquisition/NN<NNP> Corp./NN<NNP> ,/NN<,> the/NN<DT> New/NN<NNP> Jersey/NN<NNP> investor/NN 's/NN<POS> acquisition/NN vehicle/NN ,/NN<,> said/NN<VBD> that/NN<IN> as/NN<IN> of/NN<IN> the/NN<DT> close/NN of/NN<IN> business/NN yesterday/NN ,/NN<,> 560,839/NN<CD> shares/NN<NNS> had/NN<VBD> been/NN<VBN> tendered/NN<VBN> ./NN<.> \n",
            "\n",
            "(4) Including/DT<VBG> the/NN<DT> stake/NN DIG/NN<NNP> already/NN<RB> held/NN<VBD> ,/NN<,> DIG/NN<NNP> holds/NN<VBZ> a/NN<DT> total/NN of/NN<IN> about/NN<RB> 25/NN<CD> %/NN of/NN<IN> Di/NN<NNP> Giorgio/NN<NNP> 's/NN<POS> shares/NN<NNS> on/NN<IN> a/NN<DT> fully/NN<RB> diluted/NN<VBN> basis/NN ./NN<.> \n",
            "\n",
            "(5) The/DT offer/NN ,/NN<,> which/NN<WDT> also/NN<RB> includes/NN<VBZ> common/NN<JJ> and/NN<CC> preferred/NN<VBN> stock/NN purchase/NN rights/NN<NNS> ,/NN<,> was/NN<VBD> to/NN<TO> expire/NN<VB> last/NN<JJ> night/NN at/NN<IN> midnight/NN ./NN<.> \n",
            "\n",
            "(6) The/IN<DT> new/IN<JJ> expiration/NN date/NN is/NN<VBZ> the/NN<DT> date/NN on/NN<IN> which/NN<WDT> \n",
            "\n",
            "(7) DIG/IN<NNP> 's/NN<POS> financing/NN<VBG> commitments/NN<NNS> ,/NN<,> which/NN<WDT> total/NN<VBP> about/NN<RB> $/NN<$> 240/NN<CD> million/NN<CD> ,/NN<,> are/NN<VBP> to/NN<TO> expire/NN<VB> ./NN<.> \n",
            "\n",
            "(8) DIG/IN<NNP> is/NN<VBZ> a/NN<DT> unit/NN of/NN<IN> DIG/NN<NNP> Holding/NN<NNP> Corp./NN<NNP> ,/NN<,> a/NN<DT> unit/NN of/NN<IN> Rose/NN<NNP> Partners/NN<NNP> L.P/NN<NNP> ./NN<.> \n",
            "\n",
            "(9) Mr./IN<NNP> Goldberg/IN<NNP> is/NN<VBZ> the/NN<DT> sole/NN<JJ> general/NN<JJ> partner/NN in/NN<IN> Rose/NN<NNP> Partners/NN<NNP> ./NN<.> \n",
            "\n",
            "(10) In/DT<IN> August/NN<NNP> ,/NN<,> Di/NN<NNP> Giorgio/NN<NNP> ,/NN<,> a/NN<DT> San/NN<NNP> Francisco/NN<NNP> food/NN products/NN<NNS> and/NN<CC> building/NN materials/NN<NNS> marketing/NN and/NN<CC> distribution/NN company/NN ,/NN<,> rejected/NN<VBD> Mr./NN<NNP> Goldberg/NN<NNP> 's/NN<POS> offer/NN as/NN<IN> inadequate/NN<JJ> ./NN<.> \n",
            "\n",
            "(11) In/IN New/NN<NNP> York/NN<NNP> Stock/NN<NNP> Exchange/NN<NNP> composite/NN trading/NN yesterday/NN ,/NN<,> Di/NN<NNP> Giorgio/NN<NNP> closed/NN<VBD> at/NN<IN> $/NN<$> 31.50/NN<CD> \n",
            "\n",
            "(12) a/IN<DT> share/IN<NN> ,/NN<,> down/NN<RB> $/NN<$> 1.75/NN<CD> ./NN<.> \n",
            "\n",
            "(13) What/IN<WP> does/NN<VBZ> n't/NN<RB> belong/NN<VB> here/NN<RB> ?/NN<.> \n",
            "\n",
            "(14) A./IN<LS> manual/NN<JJ> typewriters/NN<NNS> ,/NN<,> \n",
            "\n",
            "(15) B./IN<LS> black-and-white/NN<JJ> snapshots/NN<NNS> ,/NN<,> \n",
            "\n",
            "(16) C./IN<LS> radio/NN adventure/NN shows/NN<NNS> ./NN<.> \n",
            "\n",
            "(17) If/IN you/IN<PRP> guessed/NN<VBD> black-and-white/NN<JJ> snapshots/NN<NNS> ,/NN<,> you/NN<PRP> 're/NN<VBP> right/NN<JJ> ./NN<.> \n",
            "\n",
            "(18) After/IN years/NN<NNS> of/NN<IN> fading/NN into/NN<IN> the/NN<DT> background/NN ,/NN<,> two-tone/NN<JJ> photography/NN is/NN<VBZ> coming/NN<VBG> back/NN<RB> ./NN<.> \n",
            "\n",
            "(19) Trendy/DT<JJ> magazine/NN advertisements/NN<NNS> feature/NN<VBP> stark/NN<JJ> black-and-white/NN<JJ> photos/NN<NNS> of/NN<IN> Hollywood/NN<NNP> celebrities/NN<NNS> pitching/NN<VBG> jeans/NN<NNS> ,/NN<,> shoes/NN<NNS> and/NN<CC> liquor/NN ./NN<.> \n",
            "\n",
            "(20) Portrait/IN<NN> studios/NN<NNS> accustomed/NN<VBN> to/NN<TO> shooting/NN<VBG> only/NN<RB> in/NN<IN> color/NN report/NN<VBP> a/NN<DT> rush/NN to/NN<TO> black-and-white/NN<JJ> portrait/NN orders/NN<NNS> ./NN<.> \n",
            "\n",
            "Test Accuracy:  0.14130958444025382\n",
            "Program ends.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "g9h5VCxA74km"
      }
    }
  ]
}